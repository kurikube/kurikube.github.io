[{"categories":["first"],"contents":"0. 前提 1. 動作環境   CodePen See the Pen VEYRqX by kurikube (@kurikube) on CodePen.     Steam     Gits\n   . 最後に . 参考にしたサイト  https://foresuke.com/post/hugo_embed/  ","date":"2022-02-02T11:30:03Z","image":null,"permalink":"https://kurikube.github.io/post/codepen/","tags":["first"],"title":"Codepen埋め込み"},{"categories":["first"],"contents":"背景 業務でCentOSをインストールしたGPUサーバの\n CPU Memory Disk GPU  の使用率を測定する必要があったので、その際に行った作業のメモを残します\nCPU・Memory  sarコマンドのインストール  sudo yum -y install sysstat  CPU・Memoryの使用率取得  sar -P ALL 1 300 \u0026gt; cpu.out #1秒間隔で300回測定→5分測定 sar -r 1 300 \u0026gt; mem.out #1秒間隔で300回測定→5分測定 ディスク容量 ※調べたいパス毎 for i in {1..300} ; do du -sh \u0026lt;ファイルパス\u0026gt;; sleep 1 ; done \u0026gt; disk.out GPU使用率 以下をそれぞれ整形し、ファイルに出力するようにします\n プロセッサ・メモリ  nvidia-smi -l 1 --query-gpu=timestamp,index,utilization.gpu,utilization.memory --format=csv,nounits  エンコーダ・デコーダ  nvidia-smi -q | grep -v Stats | grep -e Encoder -e Decoder | xargs -L 1 echo `date +'%Y/%m/%d %I:%M:%S',` ","date":"2022-01-31T11:30:03Z","image":null,"permalink":"https://kurikube.github.io/post/resource_sokutei/","tags":["first"],"title":"CentOSのCPU・Memory・Disk・GPU使用率を測定する"},{"categories":null,"contents":" CentOSにDockerをインストールするためのシェルスクリプトになります。 root権限で実行が楽です。セキュリティ的によく無いですが。  yum -y update yum -y upgrade yum install -y yum-utils device-mapper-persistent-data lvm2 yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo yum install -y docker-ce docker-ce-cli containerd.io systemctl start docker systemctl enable docker ","date":"2022-01-31T11:30:03Z","image":null,"permalink":"https://kurikube.github.io/post/20220130/","tags":["Docker","CentOS"],"title":"DockerをCentOSにインストールするためのシェルスクリプト"},{"categories":["Linux","kubernetes"],"contents":"0. 前提  kubernetes, Operatorに関する初歩的な知識があるとなお良い  1. 動作環境  EC2 (t3.2xlarge) Ubuntu 20.04.2 LTS Docker 20.10.7 go 1.17.6  2. Dockerでjaegerをインストールする  Jaegerインストール済みのDockerコンテナを起動する  docker run -d --name=jaeger -p 5775:5775/udp -p 16686:16686 jaegertracing/all-in-one:latest http://localhost:16686 でUIにアクセス可能です。\n3. GoでJaegerにTrace情報を連携する Trace情報を渡すプログラムをGoで実装してみます。\n ライブラリをインストールする  go get github.com/uber/jaeger-client-go go get github.com/opentracing/opentracing-go go get github.com/pkg/errors  Goのブログラムを実装する trace_sampleという名前のTracerを作成し、そのTracerの中にhelloとworldの２種類のspanが流れるサンプルコードになっています。  package main import ( \u0026quot;fmt\u0026quot; \u0026quot;github.com/opentracing/opentracing-go\u0026quot; \u0026quot;github.com/uber/jaeger-client-go\u0026quot; \u0026quot;github.com/uber/jaeger-client-go/config\u0026quot; \u0026quot;time\u0026quot; ) func main() { cfg := config.Configuration{ Sampler: \u0026amp;config.SamplerConfig{ Type: \u0026quot;const\u0026quot;, Param: 1, }, Reporter: \u0026amp;config.ReporterConfig{ LogSpans: true, BufferFlushInterval: 1 * time.Second, LocalAgentHostPort: \u0026quot;127.0.0.1:5775\u0026quot;, //先程作成したDockerコンテナの5775ポートをレポート先ホストとして指定して設定(UDP通信) }, } tracer, closer, err := cfg.New( \u0026quot;trace_sample\u0026quot;, config.Logger(jaeger.StdLogger), ) if err != nil { fmt.Println(err) } opentracing.SetGlobalTracer(tracer) //Tracerを作成 defer closer.Close() someFunction() } func someFunction() { parent := opentracing.GlobalTracer().StartSpan(\u0026quot;hello\u0026quot;) // helloという名前のspanを作成 defer parent.Finish() child := opentracing.GlobalTracer().StartSpan( \u0026quot;world\u0026quot;, opentracing.ChildOf(parent.Context())) // worldという名前のspanを作成 defer child.Finish() }  実行する 下記のエラーが発生しました。go.modが無いと怒られているようなので、作成します。  jaeger_sample.go:5:5: no required module provides package github.com/opentracing/opentracing-go: go.mod file not found in current directory or any parent directory; see \u0026#39;go help modules\u0026#39; jaeger_sample.go:6:5: no required module provides package github.com/uber/jaeger-client-go: go.mod file not found in current directory or any parent directory; see \u0026#39;go help modules\u0026#39; jaeger_sample.go:7:5: no required module provides package github.com/uber/jaeger-client-go/config: go.mod file not found in current directory or any parent directory; see \u0026#39;go help modules\u0026#39;  go.modを作成する  go mod init test3  またエラーが発生するので、指示に従う  go run jaeger_sample.go jaeger_sample.go:5:5: no required module provides package github.com/opentracing/opentracing-go; to add it: go get github.com/opentracing/opentracing-go jaeger_sample.go:6:5: no required module provides package github.com/uber/jaeger-client-go; to add it: go get github.com/uber/jaeger-client-go jaeger_sample.go:7:5: no required module provides package github.com/uber/jaeger-client-go/config; to add it: go get github.com/uber/jaeger-client-go/config go get github.com/opentracing/opentracing-go go get github.com/uber/jaeger-client-go/config go get github.com/uber/jaeger-client-go  もう一度、実行する   go run jaeger_sample.go 2022/01/30 07:24:43 debug logging disabled 2022/01/30 07:24:43 Initializing logging reporter 2022/01/30 07:24:43 debug logging disabled 2022/01/30 07:24:43 Reporting span 4910416a0c1a18e3:0b0f7921aa631b9b:4910416a0c1a18e3:1 2022/01/30 07:24:43 Reporting span 4910416a0c1a18e3:4910416a0c1a18e3:0000000000000000:1 うまくいったようですね。\n4. UIを確認する Serviceの欄に先程のコード内で指定したサービス名「trace_sample」が出てきます。 Find tracesを実行するとtrace_sampleのサービスで記録されたTrace情報が1件出てきます。 この情報の詳細をチェックすると以下の図のように、helloとworldの2つのspanが登録されていることがわかります。\n5. 最後に 今回はdockerで動かしましたが、今度はk8sでもできるかやってみます。\n6. 参考にしたサイト  https://qiita.com/ike_dai/items/f7d95852a86a46e1f19d  ","date":"2022-01-30T01:30:03Z","image":null,"permalink":"https://kurikube.github.io/post/jaeger/","tags":["Jaeger"],"title":"DockerでJaegerを動かす(2022/1/30)"},{"categories":["Hugo","Papermod"],"contents":"やることは2つあります。 1. config.yamlの`menu.mainにcategoriesを追加します menu: main: - identifier: categories name: categories url: /categories/ weight: 10 2. archetypes/default.mdのtagsの下に、categories: [\u0026quot;Hugo\u0026quot;,\u0026quot;Papermod\u0026quot;]のような感じで記載すします。tagsと同じ感じですね。 ","date":"2022-01-29T11:30:03Z","image":null,"permalink":"https://kurikube.github.io/post/20220101/","tags":[""],"title":"Hugo + Papermod でカテゴリで分類する方法"},{"categories":["Linux"],"contents":"0.前提  kubernetes, helmに関する初歩的な知識  1.動作環境  EC2 (t3.xlarge) Ubuntu 20.04.03 LTS kind version 0.11.1 Docker 20.10.7 helm v3.8.0 go 1.17  2. KEDAをHelmでインストールする  https://keda.sh/docs/2.5/deploy/ を参考に、インストールする  2.1 Helm リポジトリを追加する ubuntu@ip-172-31-27-87:~$ helm repo add kedacore https://kedacore.github.io/charts \u0026#34;kedacore\u0026#34; has been added to your repositories 2.2 Helmのリポジトリをアップデートする ubuntu@ip-172-31-27-87:~$ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026#34;kedacore\u0026#34; chart repository Update Complete. ⎈Happy Helming!⎈ 2.3 kedaをhelmチャートでインストールする ubuntu@ip-172-31-27-87:~$ kubectl create namespace keda namespace/keda created ubuntu@ip-172-31-27-87:~$ helm install keda kedacore/keda --namespace keda NAME: keda LAST DEPLOYED: Sun Jan 30 05:15:51 2022 NAMESPACE: keda STATUS: deployed REVISION: 1 TEST SUITE: None 2.4 kedaでインストールされたコンポーネントを確認する ubuntu@ip-172-31-27-87:~$ kubectl get all -n keda NAME READY STATUS RESTARTS AGE pod/keda-operator-5748df494c-qfk8c 1/1 Running 0 2m13s pod/keda-operator-metrics-apiserver-cb649dd48-lkxtw 1/1 Running 0 2m13s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/keda-operator-metrics-apiserver ClusterIP 10.96.44.179 \u0026lt;none\u0026gt; 443/TCP,80/TCP 2m13s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/keda-operator 1/1 1 1 2m13s deployment.apps/keda-operator-metrics-apiserver 1/1 1 1 2m13s NAME DESIRED CURRENT READY AGE replicaset.apps/keda-operator-5748df494c 1 1 1 2m13s replicaset.apps/keda-operator-metrics-apiserver-cb649dd48 1 1 1 2m13s サンプルアプリをデプロイする 3.1 RabbitMQ https://github.com/kedacore/sample-go-rabbitmq を参考に、サンプルアプリをkindクラスタにデプロイしてみます。\n手順は以下の通り\n サンプルアプリのgitリポジトリをcloneする helmでRabbitMQのキューを作成する RabbitMQのコンシューマーをデプロイする メッセージをキューにパブリッシュする (オプション)お片付け  3.1.1 サンプルアプリのgitリポジトリをcloneする git clone https://github.com/kedacore/sample-go-rabbitmq cd sample-go-rabbitmq 3.1.2 helmでRabbitMQのキューを作成する Bitnamiのリポジトリを追加する\nhelm repo add bitnami https://charts.bitnami.com/bitnami helm v3かつkindを利用しているので、下記の方法でインストールする\nhelm install rabbitmq --set auth.username=user --set auth.password=PASSWORD --set volumePermissions.enabled=true bitnami/rabbitmq 3.1.3 RabbitMQがデプロイされたことを確認する kubectl get po NAME READY STATUS RESTARTS AGE rabbitmq-0 1/1 Running 0 50s 3.1.4 RabbitMQのコンシューマーをデプロイする kubectl apply -f deploy/deploy-consumer.yaml デプロイの状態を確認する\nkubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE rabbitmq-consumer 0/0 0 0 32s Podがひとつも起動していないことが分かる。これはRabbit MQのキューが一つもないからです。Podが増えるかこれから確認してみます。\n3.1.5 メッセージをキューにパブリッシュする kubectl apply -f deploy/deploy-publisher-job.yaml デプロイメントの状態を確認する。下からも分かるように次第にPodが増加しています。\nkubectl get deploy -w kubectl get deploy -w NAME READY UP-TO-DATE AVAILABLE AGE rabbitmq-consumer 1/1 1 1 4m26s kubectl get deploy -w NAME READY UP-TO-DATE AVAILABLE AGE rabbitmq-consumer 4/4 4 4 4m36s kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE rabbitmq-consumer 8/16 16 8 5m3s 下記のコマンドで、Podあたりの処理するメッセージ数を確認できます。\nkubectl get hpa キューが空になり、一定の時間が経過するとPodは０にスケールダウンします。\nkubectl get deploy -w NAME READY UP-TO-DATE AVAILABLE AGE rabbitmq-consumer 0/0 0 0 7m25s 3.1.6 (オプション)お片付け kubectl delete job rabbitmq-publish kubectl delete ScaledObject rabbitmq-consumer kubectl delete deploy rabbitmq-consumer helm delete rabbitmq 最後に 今回はKEDAをインストールし、実際にサンプルアプリをデプロイしてみました。kubernetesのサーバレスアプリである、Knativeに関してもKnativeでサーバレスを実現するで触れているので良かったら読んでみて下さい。\n","date":"2022-01-29T11:30:03Z","image":null,"permalink":"https://kurikube.github.io/post/keda/","tags":["kinc"],"title":"KEDA v2をkindで動かしてみる"},{"categories":["サーバレス","Linux"],"contents":"0. 前提  kubernetes, Dockerに関する基礎知識  1. 動作環境  EC2(t3.xlarge) (※補足 t3.largeだとcpu不足で動作しませんでした。) Ubuntu  NAME=\u0026#34;Ubuntu\u0026#34; VERSION=\u0026#34;20.04.3 LTS (Focal Fossa)\u0026#34; ID=ubuntu ID_LIKE=debian PRETTY_NAME=\u0026#34;Ubuntu 20.04.3 LTS\u0026#34; VERSION_ID=\u0026#34;20.04\u0026#34; HOME_URL=\u0026#34;https://www.ubuntu.com/\u0026#34; SUPPORT_URL=\u0026#34;https://help.ubuntu.com/\u0026#34; BUG_REPORT_URL=\u0026#34;https://bugs.launchpad.net/ubuntu/\u0026#34; PRIVACY_POLICY_URL=\u0026#34;https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\u0026#34; VERSION_CODENAME=focal UBUNTU_CODENAME=focal  kind version 0.11.1 knative-serving v0.29.0 kn-quickstart Version: v1.2.0 Docker 20.10.7 npm v6.14.4  2. 試してみる 2.1 kind, kubectlのインストール  kindのインストール  https://kind.sigs.k8s.io/docs/user/quick-start/ を参考にインストールする\ncurl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.11.1/kind-linux-amd64 chmod +x ./kind mv ./kind /usr/local/bin/kind  kubectlのインストール https://kubernetes.io/ja/docs/tasks/tools/install-kubectl/ を参考にインストール  sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y apt-transport-https gnupg2 curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - echo \u0026#34;deb https://apt.kubernetes.io/ kubernetes-xenial main\u0026#34; | sudo tee -a /etc/apt/sources.list.d/kubernetes.list sudo apt-get update sudo apt-get install -y kubectl 2.2 kn, kn-quickstartコマンドのインストール  knコマンドのインストール https://github.com/knative/client/releases から拾ってくる  wget https://github.com/knative/client/releases/download/knative-v1.2.0/kn-linux-amd64 chmod +x kn-linux-amd64 mv ./kn-linux-amd64 /usr/local/bin/kn  kn-quickstartコマンドのインストール https://github.com/knative-sandbox/kn-plugin-quickstart/releases からダウンロードする  wget https://github.com/knative-sandbox/kn-plugin-quickstart/releases/download/knative-v1.2.0/kn-quickstart-linux-amd64 chmod +x kn-quickstart-linux-amd64 mv ./kn-quickstart-linux-amd64 /usr/local/bin/kn-quickstart 2.3 npm, nodejsのインストール  npmのインストール  sudo apt install npm npm -v  nodejsのインストール  sudo apt install nodejs nodejs -v 2.4 knativeクラスタを立ち上げる kn quickstart kind kind get clusters 2.5 サンプルアプリケーションをデプロイする https://github.com/knative/docs/tree/main/code-samples/serving/hello-world/helloworld-nodejs　を参考にサンプルアプリをデプロイする nodejsプロジェクトを作成し、expressをインストールする\nmkdir knative-service-sample cd knative-service-sample npm init -y npm install express index.jsを作成する。index.jsの中身は下のようになる。\nconst express = require('express'); const app = express(); app.get('/', (req, res) =\u0026gt; { console.log('Hello world received a request.'); const target = process.env.TARGET || 'World'; res.send(`Hello ${target}!\\n`); }); const port = process.env.PORT || 8080; app.listen(port, () =\u0026gt; { console.log('Hello world listening on port', port); }); package.jsonを編集する。内容を下記のように編集する\n{ \u0026quot;name\u0026quot;: \u0026quot;knative-serving-helloworld\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;1.0.0\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;Simple hello world sample in Node\u0026quot;, \u0026quot;main\u0026quot;: \u0026quot;index.js\u0026quot;, \u0026quot;scripts\u0026quot;: { \u0026quot;start\u0026quot;: \u0026quot;node index.js\u0026quot; }, \u0026quot;author\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;license\u0026quot;: \u0026quot;Apache-2.0\u0026quot;, \u0026quot;dependencies\u0026quot;: { \u0026quot;express\u0026quot;: \u0026quot;^4.16.4\u0026quot; } } Dockerfileを作成する。下記のように記載する\n# Use the official lightweight Node.js 12 image. # https://hub.docker.com/_/node FROM node:12-slim # Create and change to the app directory. WORKDIR /usr/src/app # Copy application dependency manifests to the container image. # A wildcard is used to ensure both package.json AND package-lock.json are copied. # Copying this separately prevents re-running npm install on every code change. COPY package*.json ./ # Install production dependencies. RUN npm install --only=production # Copy local code to the container image. COPY . ./ # Run the web service on container startup. CMD [ \u0026quot;npm\u0026quot;, \u0026quot;start\u0026quot; ] .dockerigonerファイルを記載する\nDockerfile README.md node_modules npm-debug.log service.yamlを編集する usernameの部分は各自のDockerhubのユーザー名に変更する\napiVersion: serving.knative.dev/v1 kind: Service metadata: name: helloworld-nodejs namespace: default spec: template: spec: containers: - image: docker.io/{username}/helloworld-nodejs env: - name: TARGET value: \u0026quot;Node.js Sample v1\u0026quot; Dockerイメージをビルドする\ndocker build -t {username}/helloworld-nodejs . docker push {username}/helloworld-nodejs k8sクラスタにデプロイする\nkubectl apply --filename service.yaml サービスのエンドポイントを確認する\nkubectl get ksvc helloworld-nodejs --output=custom-columns=NAME:.metadata.name,URL:.status.url アプリにアクセスしてみる。\ncurl http://helloworld-nodejs.default.1.2.3.4.sslip.io 無事にデプロイできると最初はSTATUSがRunningだが途中からTerminatingになることが分かる。 別のターミナルでkubectl get po -wしながら、curl http://helloworld-nodejs.default.1.2.3.4.sslip.io してみると再度Podが立ち上がる。\n3. クラスタを片付ける Knativeはかなりリソースを消費するので、今後使用しない場合はクラスタを削除しましょう。\nkind delete clusters knative 4.最後に 今回はknativeでサーバレスを試してみました。knative以外にも、kubernetesではKEDAでサーバレスを実現できるようです。 こちらも記事（ KEDA v2をkindで動かしてみる　）にしているので時間がある時に読んでみて下さい。\n5. 参考にした記事  https://qiita.com/t_okkan/items/eef036534ce3d5511df6  ","date":"2022-01-29T11:30:03Z","image":null,"permalink":"https://kurikube.github.io/post/knative/","tags":["kubernetes","knative"],"title":"Knativeでサーバレスを実現する"},{"categories":["Linux"],"contents":"はじめに OpenShift Interactive Learning Portal の Deploying Applications From Images コースをCodeReady Container環境で行う。\nhttps://learn.openshift.com/introduction/deploying-images/\n学習内容 OpenShiftを使用する場合、アプリケーションを追加する方法はいくつかある。 主な方法は次のとおり。\n 既存のコンテナイメージからアプリケーションをデプロイする Source-to-Imageビルダーを使用して、Gitリポジトリーに含まれているソースコードからビルドおよびデプロイする DockerfileのGitリポジトリに含まれるソースコードからビルドしてデプロイする  このコースでは、OpenShift Webコンソールおよびocコマンドをを使用して、既存のコンテナイメージからアプリケーションをデプロイする方法を学習する。\n環境 以下で作成したCodeReady Container環境を使用 http://10.88.10.20:8080/open.knowledge/view/40?offset=0\nDeploying Using the Command Line developerユーザでログインする。\n[crc@codeready ~]$ oc login -u developer -p developer Login successful. projectは以前作成した「myproject」を使用する。\n[crc@codeready ~]$ oc project Using project \u0026#34;myproject\u0026#34; on server \u0026#34;https://api.crc.testing:6443\u0026#34;. デプロイするimageの名前を指定して、それがコマンドラインから有効かどうかを確認するには、oc new-app \u0026ndash;searchコマンドを使用する。\n→以下の表示結果からは、Docuer Hubレジストリにimageがあることが分かる。\n[crc@codeready ~]$ oc new-app --search openshiftkatacoda/blog-django-py Docker images (oc new-app --docker-image=\u0026lt;docker-image\u0026gt; [--code=\u0026lt;source\u0026gt;]) ----- openshiftkatacoda/blog-django-py Registry: Docker Hub Tags: latest imageをデプロイするには次のコマンドを実行する。\n[crc@codeready ~]$ oc new-app openshiftkatacoda/blog-django-py --\u0026gt; Found container image 927f823 (9 months old) from Docker Hub for \u0026#34;openshiftkatacoda/blog-django-py\u0026#34; Python 3.5 ---------- Python 3.5 available as container is a base platform for building and running various Python 3.5 applications and frameworks. Python is an easy to learn, powerful programming language. It has efficient high-level data structures and a simple but effective approach to object-oriented programming. Python\u0026#39;s elegant syntax and dynamic typing, together with its interpreted nature, make it an ideal language for scripting and rapid application development in many areas on most platforms. Tags: builder, python, python35, python-35, rh-python35 * An image stream tag will be created as \u0026#34;blog-django-py:latest\u0026#34; that will track this image * This image will be deployed in deployment config \u0026#34;blog-django-py\u0026#34; * Port 8080/tcp will be load balanced by service \u0026#34;blog-django-py\u0026#34; * Other containers can access this service through the hostname \u0026#34;blog-django-py\u0026#34; --\u0026gt; Creating resources ... imagestream.image.openshift.io \u0026#34;blog-django-py\u0026#34; created deploymentconfig.apps.openshift.io \u0026#34;blog-django-py\u0026#34; created service \u0026#34;blog-django-py\u0026#34; created --\u0026gt; Success Application is not exposed. You can expose services to the outside world by executing one or more of the commands below: \u0026#39;oc expose svc/blog-django-py\u0026#39; Run \u0026#39;oc status\u0026#39; to view your app. OpenShiftは、imageの名前に基づいてデフォルト名を割り当てる（この場合はblog-django-py）。引数として\u0026ndash;nameオプションを指定することにより、アプリケーションと作成されるリソースに異なる名前を指定できる。\nWebコンソールから既存のコンテナーイメージをデプロイする場合とは異なり、アプリケーションはデフォルトでOpenShiftクラスターの外部に公開されない。 OpenShiftクラスターの外部で使用できるように作成されたアプリケーションを公開するには、次のコマンドを実行する。\n[crc@codeready ~]$ oc expose service/blog-django-py route.route.openshift.io/blog-django-py exposed 作成したrouteについては、WebコンソールからもCLIからも確認できる。\n  Webコンソールから確認\nコンソールを選択してOpenShift Webコンソールに切り替え、アプリケーションがデプロイされていることを確認する。 トポロジビューでアプリケーションに表示されるURLショートカットアイコンを選択して、プロジェクトにアクセスする。\n  コマンドラインから確認\nコマンドラインから作成されたルートに割り当てられているホスト名を表示するには、次のコマンドを実行する。\n  [crc@codeready ~]$ oc get route/blog-django-py NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD blog-django-py blog-django-py-myproject.apps-crc.testing blog-django-py 8080-tcp None ブラウザからURLへアクセスすると作成したサイトへアクセスできた。\nhttp://blog-django-py-myproject.apps-crc.testing/\nコースはここで終了だが、oc new-appコマンドで何が作成されたかを以下で確認しておく。\n[crc@codeready ~]$ oc get all NAME READY STATUS RESTARTS AGE pod/blog-django-py-1-build 1/1 Running 0 3m50s pod/blog-django-py-1-deploy 0/1 Completed 0 27m pod/blog-django-py-1-n7fvs 1/1 Running 0 27m NAME DESIRED CURRENT READY AGE replicationcontroller/blog-django-py-1 1 1 1 27m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/blog-django-py ClusterIP 172.30.52.157 \u0026lt;none\u0026gt; 8080/TCP 27m NAME REVISION DESIRED CURRENT TRIGGERED BY deploymentconfig.apps.openshift.io/blog-django-py 1 1 1 config,image(blog-django-py:latest) NAME TYPE FROM LATEST buildconfig.build.openshift.io/blog-django-py Source Git 1 NAME TYPE FROM STATUS STARTED DURATION build.build.openshift.io/blog-django-py-1 Source Git@35b89e2 Running 3 minutes ago NAME IMAGE REPOSITORY TAGS UPDATED imagestream.image.openshift.io/blog-django-py default-route-openshift-image-registry.apps-crc.testing/myproject/blog-django-py latest 27 minutes ago NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD route.route.openshift.io/blog-django-py blog-django-py-myproject.apps-crc.testing blog-django-py 8080-tcp None deploymentconfig, buildconfig, build, imagestreamについての学習はまた別途。\n","date":"2022-01-29T11:30:03Z","image":null,"permalink":"https://kurikube.github.io/post/crc_openshift_interactive_learning_portal/","tags":["OpenShift"],"title":"OpenShift Interactive Learning Portal ①Deploying Applications From Images"},{"categories":["Linux"],"contents":"0. 前提  minikube, Docker, Nginxに関する基礎的な知識があるとなお良いです  1. 動作環境  EC2(t3.2xlarge) Ubuntu 20.04.2 Docker 20.10.9 minikube v1.23.2 Nginx 1.18.0  2. minikube start --driver=docker --apiserver-ips=172.31.24.235 sudo apt update sudo apt install nginx sudo ufw allow from 0.0.0.0/0 to any port 51999 sudo ufw status . 最後に . 参考にしたサイト  https://zepworks.com/posts/access-minikube-remotely-kvm/  ","date":"2022-01-29T11:30:03Z","image":null,"permalink":"https://kurikube.github.io/post/minikube_remote/","tags":["minikube","kubernetes"],"title":"リモート環境に立てたMinikubeで公開しているロードバランサーを外部から利用可能にする"},{"categories":null,"contents":"背景 CentOS7を新規インストール時、不要なパッケージを入れたくなかったので「Minimal ISO」のイメージを使用しました。当然ながらGUI環境が入ってなかったので、GUIを使えるように、追加でインストールを行いました。\nGUI環境のインストール まずはGUI環境を以下のコマンドでインストールします。「groupinstall」を使えば、関連するパッケージ群を1つのグループとしてまとめてインストール可能です。\nsudo yum -y groupinstall \u0026#34;GNOME Desktop\u0026#34; GUIを有効化する GUI環境一式のインストール完了したら、以下のコマンドでGUIを有効にします。\nstartx 有効化すると以下のような初期設定画面が表示されるので、言語設定などをします。 初期設定が完了すると、GUI環境で作業ができるようになる。 GUI環境で起動するようにする GUI環境は整ったが、このままではシステムを再起動するとCUIに戻ってしまいます。\n次回起動時もGUI環境で立ち上げたい場合は、システム起動時のランレベルを変えてあげる必要があります。ランレベルは簡単に言えば「起動モード」みたいなものです。\nsudo systemctl set-default graphical.target これで次回起動時からGUI環境で立ち上がるようになります。ちなみに、「やっぱり起動時はCUIにしたい」というときは以下のコマンドで元に戻せます。\nsudo systemctl set-default multi-user.target ","date":"2022-01-28T12:30:03Z","image":null,"permalink":"https://kurikube.github.io/post/20220128/","tags":["Centos","GUI"],"title":"最小インストールしたCentOS7にGUIを導入する"},{"categories":null,"contents":"CRC VMの内部動作を確認する CRC VMのIPアドレスを確認する\n[centos@ip-172-31-19-74 ~]$ crc ip 192.168.130.11 CRC VMへSSH接続する\n[centos@ip-172-31-19-74 ~]$ ssh -i ~/.crc/machines/crc/id_ecdsa core@$CRC_IP CRC VMの中でpodmanでdnsmasq(DNSサーバ)が起動していることが分かる\n[core@crc-xxcfw-master-0 ~]$ sudo podman ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 128a6fe17e5e quay.io/crcont/gvisor-tap-vsock:3231aba53905468c22e394493a0debc1a6cc6392 3 weeks ago Exited (0) 7 hours ago gvisor-tap-vsock e3593ab79e1d quay.io/crcont/dnsmasq:latest 3 weeks ago Up 7 hours ago 0.0.0.0:53-\u0026gt;53/udp crc-dnsmasq CRC実行ホストのNetwork Manager設定 CRCセットアップ後のホスト上のdnsmasqのCRC用インスタンスは下記のようになっています。\n[centos@ip-172-31-19-74 ~]$ cat /etc/NetworkManager/dnsmasq.d/crc.conf server=/apps-crc.testing/192.168.130.11 server=/crc.testing/192.168.130.11 なお、このcrc.confはcrc setupおよびcrc start時にチェックされ、設定を変更することができません\nCRCホストの/etc/hosts CRCセットアップ後、CRCを実行するホストの/etc/hostsには設定が追加されます\n[centos@ip-172-31-19-74 ~]$ sudo cat /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.130.11 api.crc.testing canary-openshift-ingress-canary.apps-crc.testing console-openshift-console.apps-crc.testing default-route-openshift-image-registry.apps-crc.testing downloads-openshift-console.apps-crc.testing oauth-openshift.apps-crc.testing リモート接続を構成する [centos@ip-172-31-19-74 ~]$ sudo yum install haproxy policycoreutils-python-utils jq [centos@ip-172-31-19-74 ~]$ yum update [centos@ip-172-31-19-74 ~]$ sudo yum install epel-release [centos@ip-172-31-19-74 ~]$ sudo yum install haproxy policycoreutils-python-utils jq [centos@ip-172-31-19-74 ~]$ yum search policycoreutils-python [centos@ip-172-31-19-74 ~]$ sudo yum install policycoreutils-python SELinuxのためのTCPポート6443のリッスンを許可します。\n[centos@ip-172-31-19-74 ~]$ sudo semanage port -a -t http_port_t -p tcp 6443 sudo cp /etc/haproxy/haproxy.cfg{,.bak} [centos@ip-172-31-19-74 ~]$ sudo cp /etc/haproxy/haproxy.cfg{,.bak}\nhaproxyのコンフィグを作成します。コンフィグ内の$CRC_IPにはCRCのIPアドレスが記載されます。\n[centos@ip-172-31-19-74 ~]$ export CRC_IP=$(crc ip) [centos@ip-172-31-19-74 ~]$ sudo tee /etc/haproxy/haproxy.cfg \u0026amp;\u0026gt;/dev/null \u0026lt;\u0026lt;EOF \u0026gt; global \u0026gt; debug \u0026gt; \u0026gt; defaults \u0026gt; log global \u0026gt; mode http \u0026gt; timeout connect 5000 \u0026gt; timeout client 500000 \u0026gt; timeout server 500000 \u0026gt; \u0026gt; frontend apps \u0026gt; bind 0.0.0.0:80 \u0026gt; option tcplog \u0026gt; mode tcp \u0026gt; default_backend apps \u0026gt; \u0026gt; frontend apps_ssl \u0026gt; bind 0.0.0.0:443 \u0026gt; option tcplog \u0026gt; mode tcp \u0026gt; default_backend apps_ssl \u0026gt; \u0026gt; backend apps \u0026gt; mode tcp \u0026gt; balance roundrobin \u0026gt; server webserver1 $CRC_IP:80 check \u0026gt; \u0026gt; backend apps_ssl \u0026gt; mode tcp \u0026gt; balance roundrobin \u0026gt; option ssl-hello-chk \u0026gt; server webserver1 $CRC_IP:443 check \u0026gt; \u0026gt; frontend api \u0026gt; bind 0.0.0.0:6443 \u0026gt; option tcplog \u0026gt; mode tcp \u0026gt; default_backend api \u0026gt; \u0026gt; backend api \u0026gt; mode tcp \u0026gt; balance roundrobin \u0026gt; option ssl-hello-chk \u0026gt; server webserver1 $CRC_IP:6443 check \u0026gt; EOF haproxyのサービス登録とサービスの起動を行います\n[centos@ip-172-31-19-74 ~]$ sudo systemctl enable haproxy Created symlink from /etc/systemd/system/multi-user.target.wants/haproxy.service to /usr/lib/systemd/system/haproxy.service. [centos@ip-172-31-19-74 ~]$ sudo systemctl start haproxy [centos@ip-172-31-19-74 ~]$ sudo yum install dnsmasq -y [centos@ip-172-31-19-74 ~]$ sudo mv /etc/dnsmasq.conf{,.bak} [centos@ip-172-31-19-74 ~]$ sudo tee /etc/dnsmasq.conf \u0026amp;\u0026gt;/dev/null \u0026lt;\u0026lt;EOF \u0026gt; user=dnsmasq \u0026gt; group=dnsmasq \u0026gt; no-hosts \u0026gt; bind-interfaces \u0026gt; listen-address=192.168.1.25 \u0026gt; address=/testing/192.168.1.25 \u0026gt; EOF クライアント側の/etc/hostsにCRCサーバのレコードを追加する cat /etc/hosts \u0026lt;CRCサーバのIPアドレス\u0026gt; console-openshift-console.apps-crc.testing oauth-openshift.apps-crc.testing 接続してみる とりあえず証明書のエラーは無視しました。 アクセスできました。\n参考にさせて頂いた記事  https://rheb.hatenablog.com/entry/crc_remote_connecting https://crc.dev/crc/#setting-up-remote-server_gsg https://rheb.hatenablog.com/entry/2021/03/30/crc_tips https://tex2e.github.io/blog/linux/install-semanage  ","date":"2022-01-28T11:30:03Z","image":null,"permalink":"https://kurikube.github.io/post/20220124/","tags":["OpenShift"],"title":"CodeReady Containers（CRC）をリモート接続可能にする"},{"categories":null,"contents":"背景 Linuxでコマンドの実行ログを保存したいことは非常に多いと思います。これは、後で見返せるようにするためでもありますし、派遣社員などの場合は自分の身を守るための手段にもなります。\n使用方法 script \u0026lt;ログを保存したいファイル名\u0026gt; ファイル名を指定しなかった場合は、カレントディレクトリにtypescript というファイルで保存されます\n日付の入ったファイル名で保存したい場合\nscript `date +%Y%m%d`.log このようにすれば日付名で作業ログを保存できます。\n実際に使ってみます。 $ script スクリプトを開始しました、ファイルは typescript です $ date 2021年 3月 28日 日曜日 06:46:13 EDT 終了する場合はexitするかctrl+dを実行します\n$ exit exit スクリプトを終了しました、ファイルは typescript です ファイルにログが残っているか確認する\n$ cat typescript スクリプトは 2021年03月28日 06時46分11秒 に開始しました$ date 2021年 3月 28日 日曜日 06:46:13 EDT $ exit exit スクリプトは 2021年03月28日 06時47分10秒 に終了しました きちんと記録されていました。\n既存ファイルへの追記方法 次に、ファイルへの追記方法を記載します。 同じファイルに追記したい場合はオプション-aを使うことで可能です。\nログファイルをlessコマンドで綺麗に読むためには scriptコマンドで取得したログにエスケープシーケンスが含まれる場合、普通のテキストとして読むとエスケープシーケンスがESCのように表示され可読性が非常に落ちます。この場合、\nless -R で読むと綺麗に読むことができます。\n参考記事  https://qiita.com/networkelements/items/aa4aec5417306b4c7e71 https://mk-55.hatenablog.com/entry/2017/05/21/013930  ","date":"2022-01-28T11:30:03Z","image":null,"permalink":"https://kurikube.github.io/post/20210130/","tags":["script"],"title":"scriptコマンドでコマンド実行の作業ログを保存する"},{"categories":null,"contents":"背景 CentOS 7 にはデフォルトでFirefoxがインストールされてます。しかし、Google Chromeの方がより便利なこともあるので、Chromeをインストールする方法を紹介します\n環境確認 [cloudg@localhost ~]$ cat /etc/redhat-release CentOS Linux release 7.9.2009 (Core) レポジトリファイルの作成 [cloudg@localhost ~]$ vi /etc/yum.repos.d/google.chrome.repo 下記の内容を記載する。\n[google-chrome] name=google-chrome baseurl=http://dl.google.com/linux/chrome/rpm/stable/$basearch enabled=1 gpgcheck=1 gpgkey=https://dl-ssl.google.com/linux/linux_signing_key.pub インストール [cloudg@localhost ~]$ yum install google-chrome-stable Internet の部分にGoogle Chromeが表示される 起動方法 下記コマンドで起動できる\n[cloudg@localhost ~]# google-chrome エラーが発生する場合 下記のエラーが発生することがある。\n[root@localhost ~]# google-chrome [4206:4206:0327/064251.530967:ERROR:zygote_host_impl_linux.cc(90)] Running as root without --no-sandbox is not supported. See https://crbug.com/63818 /opt/google/chrome/google-chromeの一番最後の行を変更する\nvi /opt/google/chrome/google-chrome exec -a \u0026quot;$0\u0026quot; \u0026quot;$HERE/chrome\u0026quot; \u0026quot;$@\u0026quot; --no-sandbox --user-data-dir=~ これで問題なく起動できるようになります。\n","date":"2022-01-27T11:30:03Z","image":null,"permalink":"https://kurikube.github.io/post/20220131/","tags":["Centos","GUI"],"title":"CentOS 7 にGoogle Chromeをインストールする"},{"categories":null,"contents":"0.実行した環境 NAME=\u0026#34;Ubuntu\u0026#34; VERSION=\u0026#34;20.04.3 LTS (Focal Fossa)\u0026#34; ID=ubuntu ID_LIKE=debian PRETTY_NAME=\u0026#34;Ubuntu 20.04.3 LTS\u0026#34; VERSION_ID=\u0026#34;20.04\u0026#34; HOME_URL=\u0026#34;https://www.ubuntu.com/\u0026#34; SUPPORT_URL=\u0026#34;https://help.ubuntu.com/\u0026#34; BUG_REPORT_URL=\u0026#34;https://bugs.launchpad.net/ubuntu/\u0026#34; PRIVACY_POLICY_URL=\u0026#34;https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\u0026#34; VERSION_CODENAME=focal UBUNTU_CODENAME=focal 1. インストール用のシェルスクリプト apt-get update apt-get -y install ffmpeg ffmpeg -version 2. （番外編）CentOS用のffmpegインストールスクリプト yum -y update yum -y upgrade yum -y install yum-utils yum -y install epel-release yum -y install https://download1.rpmfusion.org/free/el/rpmfusion-free-release-7.noarch.rpm yum -y install ffmpeg fmpeg-devel ","date":"2022-01-27T11:30:03Z","image":null,"permalink":"https://kurikube.github.io/post/20210129/","tags":["ffmpeg","ubuntu"],"title":"Ubuntu 20.04.3 LTSにffmpegをインストールする"},{"categories":null,"contents":"CentOSでプロキシサーバを構築する 背景  業務でインターネット接続ができないサーバから、インターネットに接続できるサーバをproxyとして外部NWと通信する必要があった。今後も似たようなことは多いと思われるのでこちらにまとめる。  Proxyとして動作させるサーバへSquidをセットアップする  yumでインストールを行う  sudo yum update sudo yum install squid バージョンを確認して正常にインストール出来ていることを確認する。  squid -v Squidの自動起動設定を行い、再起動する。  sudo systemctl enable squid sudo systemctl restart squid 4.Squidがポート3128を使用していることを確認する。\nsudo lsof -i:3128 5.出力例\nCOMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME squid 21800 squid 11u IPv6 37157 0t0 TCP *:squid (LISTEN) 6.firewallの穴あけ\nデフォルトではSquidのポートは3128が設定されており、Port番号3128へ送られるパケットは受け取れるように設定する。\nsudo firewall-cmd --zone=public --permanent --add-port=3128/tcp #許可ポートの追加 sudo firewall-cmd --reload #設定反映 sudo firewall-cmd --zone=public --list-all #結果確認(portに3128が追加されていることを確認） proxy clientへのセットアップ proxyを踏み台にプライベートIPでclientにSSHする。 それぞれの設定ファイルにプロキシの設定を記入する。\n※3128以外のポートを利用している場合は適宜変更する必要があります。\n yumの場合のproxy設定  proxy=http://\u0026lt;proxyのプライベートIP\u0026gt;:3128  wgetの場合のproxy設定  http_proxy=http://\u0026lt;proxyのプライベートIP\u0026gt;:3128 https_proxy=http://\u0026lt;proxyのプライベートIP\u0026gt;:3128  curlの場合のproxy設定  proxy=http://\u0026lt;proxyのプライベートIP\u0026gt;:3128 参考にしたサイト  https://dev.classmethod.jp/articles/squid-proxy-setup/ https://qiita.com/RyusukeKawasaki/items/04a6dc9de05ddbc8ff5d  ","date":"2022-01-26T11:30:03Z","image":null,"permalink":"https://kurikube.github.io/post/20210128/","tags":["Squid","Proxy"],"title":"SquidでProxyサーバを構築する"},{"categories":null,"contents":"0.Pelicanとは Python製の静的サイトジェネレータ。静的サイトジェネレータにも様々なものがあるが、下記の理由で今回はPelicanを利用することにした。\n 開発が活発。(GitHubのスターの数も良好。) themeも様々なものが選べる。  1.実行環境  MacBook Air (M1, 2020) m1チップ venvの仮想環境で実行 GitHub, GitHub Actions Python 3.9.9 Pelican versino 4.7.1  2.試しに利用してみる 2.0 pelican用の仮想環境を作成する venvで作成する\n下のコマンドは/Users/ユーザ名で実行しました。適宜読者の環境に合わせて変更してください。\npython3 -m venv venv 2.1 インストール Python製のパッケージであるため、pip経由でインストールする。 markdownには標準で未対応のため、Markdownを利用できるようにmarkdownのパッケージをインストールする。 (省略可能)GitHub Pagesへの操作を可能にするためghp-importをインストールする\npip install pelican markdown ghp-import 2.2 Pelican環境構築 pelican-quickstart を実行すると対話形式でセッティングが始まる。\n2.3記事を記載する content ディレクトリ以下にmarkdownで記事をかく。\n記事の先頭にはpelicanの決まりでいくつか内容を記載し、その下に投稿したい内容を記載する\n2.4 記事を生成する markdownを作成したら、下記のコマンドでHTMLファイルを生成する\nmake html このコマンドで、手元のローカル環境用のファイルが生成される\n2.5 記事をローカルで確認する make serve http://localhost:8000 開いて確認可能\n","date":"2022-01-26T01:30:03Z","image":null,"permalink":"https://kurikube.github.io/post/20210127/","tags":["Pelican","GitHub"],"title":"Pelicanを使って静的サイトを構築する"},{"categories":null,"contents":"0.CCNP取得前の私のスキル  地方の国立大学を卒業。（一応理系） 縁あって通信企業に入社。 CCNAを2019に取得。この頃は実機試験もありました。  1.なぜCCNPを取得しようと考えたか？ ","date":"2022-01-11T11:30:03Z","image":null,"permalink":"https://kurikube.github.io/post/20210126/","tags":["CCNP","Cisco","資格"],"title":"CCNP合格したので、勉強法まとめて見た"},{"categories":["Linux"],"contents":"概要 OpenShift4.x以降はMinishiftがなくなり、OpenShift CodeReady Container（CRC）となっている。 以下では公式ドキュメントに沿って手元のベアメタルサーバへインストールを行う。このサーバで色々なことを試してみる。 https://crc.dev/crc/\n環境  ホストOS：CentOS7.7 構成図 マシン上でCRCをインストールすると自動的にNWが作成されてCRCが作成される 。 OpenShiftのWebコンソールへアクセスする際には、ベアメタルマシンへリモデスした上でマシン上のブラウザからアクセスする必要がある。 環境確認  [root@ip-172-31-19-74 ~]# cat /etc/redhat-release CentOS Linux release 7.7.1908 (Core) 手順 公式ドキュメントの手順に沿ってインストールする。\n1.NetworkManagerをインストールし、起動する [root@ip-172-31-19-74 ~]# yum -y install NetworkManager [root@ip-172-31-19-74 ~]# systemctl restart NetworkManager 2.資材をダウンロードして、解凍して、パスの通ったフォルダへ移動 [root@ip-172-31-19-74 ~]# yum -y install wget [root@ip-172-31-19-74 tmp]# curl -k -L https://developers.redhat.com/content-gateway/rest/mirror/pub/openshift-v4/clients/crc/latest/crc-linux-amd64.tar.xz -o /tmp/crc-linux-amd64.tar.xz #ダウンロードにかなり時間を要します。 [root@ip-172-31-19-74 ~]# cd /tmp [root@ip-172-31-19-74 tmp]# xz -dc /tmp/crc-linux-amd64.tar.xz | tar xfv - [root@ip-172-31-19-74 tmp]# cp -pi crc-linux-1.38.0-amd64/crc /usr/local/bin 3.初期セットアップ＆起動 crc startすると途中でsecretを聞かれるので、Red Hat Developer アカウントを作成して取得したsecretを入力する。完了するまで数分かかる。非rootユーザで実行する必要がある。 https://cloud.redhat.com/openshift/install/metal/user-provisioned\n[root@ip-172-31-19-74 tmp]# su - centos [centos@ip-172-31-19-74 ~]$ cd [centos@ip-172-31-19-74 ~]$ crc setup Would you like to contribute anonymous usage statistics? [y/N]: N # crcのqcow2をダウンロードするのに時間がかかります。 [centos@ip-172-31-19-74 ~]$ crc start 下記のような出力が表示される\nINFO A CodeReady Containers VM for OpenShift 4.9.12 is already running Started the OpenShift cluster. The server is accessible via web console at: https://console-openshift-console.apps-crc.testing Log in as administrator: Username: kubeadmin Password: wtXdm-cJwtQ-Jo3Ny-hKoRU Log in as user: Username: developer Password: developer Use the \u0026#39;oc\u0026#39; command line interface: $ eval $(crc oc-env) $ oc login -u developer https://api.crc.testing:6443 4.ocコマンドにパスを通す [centos@ip-172-31-19-74 ~]$ crc oc-env [centos@ip-172-31-19-74 ~]$ eval $(crc oc-env) 5.ログイン oc startしたときの出力結果に従ってoc loginする\n[centos@ip-172-31-19-74 ~]$ oc login -u developer https://api.crc.testing:6443 [centos@ip-172-31-19-74 ~]$ oc login -u kubeadmin https://api.crc.testing:6443 6.動作確認 [centos@ip-172-31-19-74 ~]$ oc get node NAME STATUS ROLES AGE VERSION crc-xxcfw-master-0 Ready master,worker 23d v1.22.3+e790d7f 7.試験用プロジェクトの作成と、busybox podの作成 Tempプロジェクトを作成してbusybox podを作成してみる。\n[centos@ip-172-31-19-74 ~]$ oc new-project temp Now using project \u0026#34;temp\u0026#34; on server \u0026#34;https://api.crc.testing:6443\u0026#34;. You can add applications to this project with the \u0026#39;new-app\u0026#39; command. For example, try: oc new-app rails-postgresql-example to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application: kubectl create deployment hello-node --image=k8s.gcr.io/serve_hostname [centos@ip-172-31-19-74 ~]$ oc run busybox --restart=Never --image=busybox pod/busybox created [centos@ip-172-31-19-74 ~]$ oc get po NAME READY STATUS RESTARTS AGE busybox 0/1 Completed 0 23s 最後に CentOSに無事CRCをインストールすることができました。これでOpenShiftについてじっくり学べますね。 最近では書籍も執筆されているようです。良かったら読んでみて下さい。\nOpenShift徹底入門  OpenShift徹底活用ガイド  ","date":"2022-01-10T11:30:03Z","image":null,"permalink":"https://kurikube.github.io/post/20210131/","tags":["OpenShift"],"title":"OpenShift Code ReadyをCentosにインストールする（2022/1/29時点）"},{"categories":["Hugo","Papermod"],"contents":"0. 背景 hugo-papermodには検索機能を使えるオプションが備わっているらしいのだが、ネットで探してもオプションの実装方法が出てこなかった。（2022/1月時点） そのため、hugoで検索機能を自分で実装することにした。\n参考にしたサイト  https://blog.mamansoft.net/2019/11/11/add-search-page-in-blog-made-by-hugo/  ","date":"2022-01-03T12:30:03Z","image":null,"permalink":"https://kurikube.github.io/post/20220103/","tags":["Linux"],"title":"hugo + Papermod で作ったwebサイトに検索機能を追加する"},{"categories":["Python","Slack"],"contents":"Slack APIとpythonを用いて一定期間より前のSlackのメッセージを削除するスクリプトの例を示す。 参考URLではlegacy Tokenを使用しているが、現在は使用できないので、OAuth Tokenを使用する必要があります。OAuth Tokenを利用するようにコードを修正しました。 また、chat.deleteメソッドとchannels.historyメソッドを使用しているが、channels.historyは使用できないので、conversations.historyメソッドを使用する必要がある。\n参考にしたURL\n https://michimani.net/post/programming-delete-old-slack-messages/  from datetime import datetime #日付を扱うため from time import sleep # 処理を一旦停止するため import json # json形式のデータを扱うため import re #正規表現の操作を行うため import sys # システムに関する処理を行うため import urllib.parse #URLを解析して構成要素を得るため import urllib.request #URLを開くため DELETE_URL = \u0026quot;https://slack.com/api/chat.delete\u0026quot; #メッセージを削除するためのSlackのAPIのURL HISTORY_URL = \u0026quot;https://slack.com/api/conversations.history\u0026quot; #メッセージの履歴を得るためのSlackのAPIのURL API_TOKEN = '*********************************' #メッセージに対する操作をする際に必要となるAPI_TOKEN※legacy Tokenは使用できないことに注意。各自で変更 TERM = 60 * 60 * 24 * 7 #1週間なので7を指定。ここの数字を変更することで、削除対象のメッセージの期間を変更できる def clean_old_message(channel_id): #メッセージを削除するための関数 print('Start cleaning message at channel \u0026quot;{}\u0026quot;.'.format(channel_id)) # どのチャンネルのメッセージを削除するのか表示する current_ts = int(datetime.now().strftime('%s')) #現在の時刻を取得 messages = get_message_history(channel_id) #channel_idで指定したchannelのメッセージを取得する for message in messages: #このfor文である期間以上より前のメッセージを自動的に削除する if current_ts - int(re.sub(r'\\.\\d+$', '', message['ts'])) \u0026gt; TERM: delete_message(channel_id, message['ts']) sleep(1) def get_message_history(channel_id): #メッセージの履歴を取得するための関数 params = { 'token': API_TOKEN, #使用するAPI_TOKENを指定する 'channel': channel_id, # メッセージ履歴を取得するchannelのidを指定する 'limit': 500 #メッセージを取得する件数の数を指定する } req_url = '{}?{}'.format(HISTORY_URL, urllib.parse.urlencode(params)) #HTTP requestを投げるためのURLを構成する req = urllib.request.Request(req_url) #実際にそのURLにrequestを投げる message_history = [] with urllib.request.urlopen(req) as res: data = json.loads(res.read().decode(\u0026quot;utf-8\u0026quot;)) #utf-8でjsonのデータをデコードする if 'messages' in data: # dataがレスポンスの文言に含まれているか確認する message_history = data['messages'] return message_history def delete_message(channel_id, message_ts): #メッセージを削除する関数 headers = { 'Content-Type': 'application/x-www-form-urlencoded' } params = { 'token': API_TOKEN, #使用するAPI_TOKENを指定する 'channel': channel_id, # メッセージ履歴を削除するchannelのidを指定する 'ts': message_ts # 削除するメッセージのタイムスタンプを指定する } req_url = '{}?{}'.format(DELETE_URL, urllib.parse.urlencode(params)) #URLを構成する文字列を定義する req = urllib.request.Request(req_url, headers=headers) # requestを投げる with urllib.request.urlopen(req) as res: data = json.loads(res.read().decode(\u0026quot;utf-8\u0026quot;)) if 'ok' not in data or data['ok'] is not True: print('Failed to delete message. ts: {}'.format(message_ts)) if __name__ == \u0026quot;__main__\u0026quot;: args = sys.argv if len(args) \u0026lt; 2: print(\u0026quot;The first parameter for slack channel id is required.\u0026quot;) else: clean_old_message(args[1]) ","date":"2022-01-02T11:30:03Z","image":null,"permalink":"https://kurikube.github.io/post/20220102/","tags":["Linux"],"title":"Slack APIを用いて一定期間より前のメッセージを自動削除する"},{"categories":null,"contents":"概要 OpenShift4.x以降はMinishiftがなくなり、OpenShift CodeReady Container（CRC）となっている。 以下では公式ドキュメントに沿って手元のベアメタルサーバへインストールを行う。このサーバで色々なことを試してみる。 https://code-ready.github.io/crc/\n環境  ホストOS：CentOS7.7 構成図 マシン上でCRCをインストールすると自動的にNWが作成されてCRCが作成される 。 OpenShiftのWebコンソールへアクセスする際には、ベアメタルマシンへリモデスした上でマシン上のブラウザからアクセスする必要がある。 環境確認  [root@localhost ~]# cat /etc/redhat-release CentOS Linux release 7.7.1908 (Core) 手順 公式ドキュメントの手順に沿ってインストールする。\n1.NetworkManagerをインストールする [root@localhost ~]# yum -y install NetworkManager 2.資材をダウンロードして、解凍して、パスの通ったフォルダへ移動 [root@localhost ~]# yum -y install wget [root@localhost ~]# curl -k -sS https://mirror.openshift.com/pub/openshift-v4/clients/crc/latest/crc-linux-amd64.tar.xz -o /tmp/crc-linux-amd64.tar.xz [root@localhost tmp]# cd /tmp [root@localhost tmp]# xz -dc /tmp/crc-linux-amd64.tar.xz | tar xfv - [root@localhost tmp]# cp -pi crc-linux-1.24.0-amd64/crc /usr/local/bin 3.初期セットアップ＆起動 crc startすると途中でsecretを聞かれるので、Red Hat Developer アカウントを作成して取得したsecretを入力する。完了するまで数分かかる。非rootユーザで実行する必要がある。 https://cloud.redhat.com/openshift/install/metal/user-provisioned\n[root@localhost tmp]# su - village [village@localhost tmp]$ crc setup [village@localhost ~]$ crc start 下記のような出力が表示される\nStarted the OpenShift cluster. The server is accessible via web console at: https://console-openshift-console.apps-crc.testing Log in as administrator: Username: kubeadmin Password: XtGFB-9dWLe-6B4sh-9ypMk Log in as user: Username: developer Password: developer Use the \u0026#39;oc\u0026#39; command line interface: $ eval $(crc oc-env) $ oc login -u developer https://api.crc.testing:6443 4.ocコマンドにパスを通す [village@localhost ~]$ crc oc-env [village@localhost ~]$ eval $(crc oc-env) 5.ログイン oc startしたときの出力結果に従ってoc loginする\n[village@localhost ~]$ oc login -u developer -p developer https://api.crc.testing:6443 6.動作確認 [village@localhost ~]$ oc get node NAME STATUS ROLES AGE VERSION crc-rsppg-master-0 Ready master,worker 8d v1.20.0+5fbfd19 いくつかのPodでエラーが出ているが、問題ないようである。\n[cloudg@localhost ~]$ oc get po --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE openshift-apiserver-operator openshift-apiserver-operator-7ddd5b5974-ncsqv 1/1 Running 0 7d18h openshift-apiserver apiserver-644ffb9d54-fdhc7 2/2 Running 0 7d18h openshift-authentication-operator authentication-operator-67c88594b5-zftcn 1/1 Running 0 7d18h openshift-authentication oauth-openshift-7cf7596786-cvmv5 1/1 Running 0 12h openshift-authentication oauth-openshift-7cf7596786-rgr9l 1/1 Running 0 12h openshift-cluster-machine-approver machine-approver-65cbf595f4-6l2nd 2/2 Running 0 7d18h openshift-cluster-node-tuning-operator cluster-node-tuning-operator-5c547f8bb4-7g4wk 1/1 Running 0 7d18h openshift-cluster-node-tuning-operator tuned-xkv6j 1/1 Running 0 8d openshift-cluster-samples-operator cluster-samples-operator-7969f7fcf5-9lrgb 2/2 Running 0 7d18h openshift-cluster-version cluster-version-operator-6bfbdf868f-sf2w2 1/1 Running 0 7d18h openshift-config-operator openshift-config-operator-57c76c77f7-45bt8 1/1 Running 0 7d18h openshift-console-operator console-operator-cb46c9d78-9f4qr 1/1 Running 0 7d18h openshift-console console-7985f9d77f-8vwpd 1/1 Running 0 8d openshift-console console-7985f9d77f-b7ds6 1/1 Running 0 8d openshift-console downloads-6df577f985-hdv4r 1/1 Running 0 7d18h openshift-controller-manager-operator openshift-controller-manager-operator-74ffd84cd9-qnf4b 1/1 Running 0 7d18h openshift-controller-manager controller-manager-z564h 1/1 Running 0 7d18h openshift-dns-operator dns-operator-6697cb89f5-z8vkx 2/2 Running 0 7d18h openshift-dns dns-default-bcvs5 3/3 Running 0 8d openshift-etcd-operator etcd-operator-664d45d68d-qb46f 1/1 Running 0 7d18h openshift-etcd etcd-crc-rsppg-master-0 3/3 Running 0 8d openshift-etcd etcd-quorum-guard-5df9d7748f-4j94p 1/1 Running 0 8d openshift-etcd revision-pruner-2-crc-rsppg-master-0 0/1 Completed 0 7d18h openshift-image-registry cluster-image-registry-operator-54778dff55-4mrlq 1/1 Running 0 7d18h openshift-image-registry image-registry-fdb76844c-hlgdz 1/1 Running 0 12h openshift-image-registry node-ca-6csl8 1/1 Running 0 8d openshift-ingress-canary ingress-canary-vtq2l 1/1 Running 0 8d openshift-ingress-operator ingress-operator-b877b674f-zhtnr 2/2 Running 0 7d18h openshift-ingress router-default-7d9fcbcfcb-w4sp5 1/1 Running 0 7d18h openshift-kube-apiserver-operator kube-apiserver-operator-64ff9b6958-mq8nd 1/1 Running 0 7d18h openshift-kube-apiserver kube-apiserver-crc-rsppg-master-0 5/5 Running 2 7d18h openshift-kube-apiserver revision-pruner-9-crc-rsppg-master-0 0/1 Completed 0 7d18h openshift-kube-controller-manager-operator kube-controller-manager-operator-577f6d5765-x8xpz 1/1 Running 0 7d18h openshift-kube-controller-manager kube-controller-manager-crc-rsppg-master-0 4/4 Running 0 8d openshift-kube-controller-manager revision-pruner-7-crc-rsppg-master-0 0/1 Completed 0 7d18h openshift-kube-scheduler-operator openshift-kube-scheduler-operator-67bcdc8d7f-dbxt2 1/1 Running 0 7d18h openshift-kube-scheduler openshift-kube-scheduler-crc-rsppg-master-0 3/3 Running 0 8d openshift-kube-scheduler revision-pruner-7-crc-rsppg-master-0 0/1 Completed 0 7d18h openshift-marketplace certified-operators-cmz6p 0/1 ImagePullBackOff 0 8d openshift-marketplace certified-operators-l288f 0/1 ImagePullBackOff 0 12h openshift-marketplace community-operators-4rzx2 0/1 ImagePullBackOff 0 8d openshift-marketplace community-operators-lw4r2 0/1 ImagePullBackOff 0 12h openshift-marketplace marketplace-operator-5f58b46865-zhhm9 1/1 Running 0 7d18h openshift-marketplace redhat-marketplace-dp4xm 0/1 ImagePullBackOff 0 12h openshift-marketplace redhat-marketplace-lgx2r 0/1 ImagePullBackOff 0 8d openshift-marketplace redhat-operators-tn9gl 0/1 ImagePullBackOff 0 12h openshift-marketplace redhat-operators-zcnzq 0/1 ImagePullBackOff 0 8d openshift-multus multus-admission-controller-bt42f 2/2 Running 0 8d openshift-multus multus-zt6dk 1/1 Running 0 8d openshift-multus network-metrics-daemon-jqhtz 2/2 Running 0 8d openshift-network-diagnostics network-check-source-7c9c5f694-wdg5x 1/1 Running 0 8d openshift-network-diagnostics network-check-target-7nvd2 1/1 Running 0 8d openshift-network-operator network-operator-cc56775cc-48j58 1/1 Running 0 7d18h openshift-oauth-apiserver apiserver-5f696fbd78-hxw6f 1/1 Running 0 8d openshift-operator-lifecycle-manager catalog-operator-599cd4d664-j9p69 1/1 Running 0 7d18h openshift-operator-lifecycle-manager olm-operator-fdc678f49-9nmnc 1/1 Running 0 7d18h openshift-operator-lifecycle-manager packageserver-54f669b87-kx6zm 1/1 Running 0 8d openshift-sdn ovs-f8djg 1/1 Running 0 8d openshift-sdn sdn-controller-pdqz8 1/1 Running 0 8d openshift-sdn sdn-dmgkt 2/2 Running 0 8d openshift-service-ca-operator service-ca-operator-78b9dcfcb7-28rvb 1/1 Running 0 7d18h openshift-service-ca service-ca-56bbf778b6-vtv8h 1/1 Running 0 8d 7.試験用プロジェクトの作成と、busybox podの作成 Tempプロジェクトを作成してbusybox podを作成してみる。\n[village@localhost ~]$ oc new-project temp Now using project \u0026#34;temp\u0026#34; on server \u0026#34;https://api.crc.testing:6443\u0026#34;. You can add applications to this project with the \u0026#39;new-app\u0026#39; command. For example, try: oc new-app rails-postgresql-example to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application: kubectl create deployment hello-node --image=k8s.gcr.io/serve_hostname [village@localhost ~]$ oc run busybox --restart=Never --image=busybox pod/busybox created [village@localhost ~]$ oc get po NAME READY STATUS RESTARTS AGE busybox 0/1 Completed 0 35s ","date":"2021-04-01T11:30:03Z","image":null,"permalink":"https://kurikube.github.io/post/20210401/","tags":["OpenShift"],"title":"OpenShift CodeReady ContainerをCentOSにインストールする（2021年時点）"},{"categories":null,"contents":"私がGitLab CI/CD(GitLab Runner)を学んだ方法 1. Qiitaの参考記事を読んでみる GitLab CI/CDパイプライン設定リファレンス の記事を一通り読んでみる →内容がボリュームありすぎて、理解できず。。\n2.GitLabの参考書籍から読んでみる Gitlab実践ガイド(インプレス)\n GitLab実践ガイド impress top gearシリーズ    これは非常にわかりやすかったです。\n","date":"2021-01-23T11:30:03Z","image":null,"permalink":"https://kurikube.github.io/post/20210124/","tags":["gitlab","gitlab runner","CI/CD","本","書籍"],"title":"GitLab CI/CDを学ぶのにおすすめの書籍"},{"categories":null,"contents":"背景  Apple M1チップ搭載のMacbookだとVirtual Boxなどの一部アプリケーションが動きません。 aws コマンドを正常にインストールできるか検証してみました。  手順  pkgファイルのダウンロード  % curl \u0026#34;https://awscli.amazonaws.com/AWSCLIV2.pkg\u0026#34; -o \u0026#34;AWSCLIV2.pkg\u0026#34; % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 25.3M 100 25.3M 0 0 10.7M 0 0:00:02 0:00:02 --:--:-- 10.7M pkgのインストール  % sudo installer -pkg AWSCLIV2.pkg -target / Password: installer: Package name is AWS Command Line Interface installer: Installing at base path / installer: The install was successful. 正常にインストールできたか確認する  % aws --version aws-cli/2.2.29 Python/3.8.8 Darwin/20.3.0 exe/x86_64 prompt/off 結論  Apple M1 チップ搭載のMacbookでもawsコマンドをインストールすることが可能でした  参考サイト  https://docs.aws.amazon.com/ja_jp/cli/latest/userguide/install-cliv2-mac.html#cliv2-mac-install-cmd  ","date":"2020-09-15T11:30:03Z","image":null,"permalink":"https://kurikube.github.io/post/20220129/","tags":["AWS","Mac"],"title":"Apple M1搭載のMacBook Airにawsコマンドをインストールする"},{"categories":null,"contents":"【初学者向け】VMware学習用のおすすめの本 業務でVMware Cloud Directorの検証をしたのですが、そこで自身のvSphere製品に関する知識の無さを痛感しました。今後同じようなVMwareの知識不足を実感する方々に向けて、参考になりそうな本を紹介させて頂きます。\nVMware徹底入門 第4版 VMware vSphere 6.0対応\n VMware徹底入門   販売元のVMwareが執筆しているということもあり、入門者向けに非常に理解しやすい内容で書かれていました。 vSphere 7.0対応の書籍の出版が待ち遠しいですね。 他にもVMware関連でわかりやすいと思った書籍があれば紹介していこうと思います。\n","date":"2020-09-15T11:30:03Z","image":null,"permalink":"https://kurikube.github.io/post/20210125/","tags":["VMware","初心者","本","書籍","vSphere"],"title":"【初心者向け】VMware (vSphere)について学ぶのにおすすめの書籍"},{"categories":null,"contents":"Hugo でyoutubeを埋め込むめのShortcode https://gohugo.io/content-management/shortcodes/#youtube にも記載のあるように、ショートコードを利用してyoutubeの動画をMarkdownに埋め込むことができます。\n例えば、https://www.youtube.com/watch?v=UUR5UFUBp5s の動画を埋め込みたい時は、Markdownの記事の中でビルトインされているショートコードを呼び出します。\n{{\u0026lt; youtube UUR5UFUBp5s \u0026gt;}} ブラウザでは以下のように出力されます。\n  参考記事  https://hugo-de-blog.com/hugo-youtube/ https://maku77.github.io/hugo/shortcode/escape.html https://blog.chick-p.work/hugo-site-directory/  ","date":"2020-01-25T11:30:03Z","image":null,"permalink":"https://kurikube.github.io/post/20200125/","tags":["hugo","youtube"],"title":"Hugoで作ったwebサイトにyoutubeの動画を埋め込む"},{"categories":null,"contents":"CentOSにDockerをインストールするためのシェルスクリプトになります。 root権限で実行が楽ですが、セキュリティ的によく無いです。\nyum -y update yum -y upgrade yum install -y yum-utils device-mapper-persistent-data lvm2 yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo yum install -y docker-ce docker-ce-cli containerd.io systemctl start docker systemctl enable docker ","date":"2020-01-24T11:30:03Z","image":null,"permalink":"https://kurikube.github.io/post/20200124/","tags":["first"],"title":"Docker install用のshell-script"},{"categories":null,"contents":"参考にしているサイト  https://bash-prompt.net/guides/custom-html-jugo/  上記のサイトの記述を参考に、hugoで作ったサイトにgoogle adsenseを入れられるか挑戦中です。\n","date":"2020-01-01T11:30:03Z","image":null,"permalink":"https://kurikube.github.io/post/20200101/","tags":["first"],"title":"Google Adsense掲載に至るまで（執筆中）"},{"categories":null,"contents":"         タイトル、本文、タグなどから記事を検索できます。\n空白区切りはOR検索になります。(AND検索はできません)\n","date":null,"image":null,"permalink":"https://kurikube.github.io/search/","tags":null,"title":"Search"}]